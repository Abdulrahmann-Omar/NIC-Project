{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkSNixeaeXgl"
      },
      "source": [
        "## Task-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U54l92YeZO1",
        "outputId": "845f73d1-3758-460b-a901-c0b8c2a397c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Authenticating with Kaggle ---\n",
            "‚úÖ Kaggle API key configured successfully.\n",
            "\n",
            "--- Downloading Sentiment140 dataset ---\n",
            "\n",
            "--- 1. Data Loading & Exploration ---\n",
            "‚úÖ Data loaded successfully.\n",
            "After removing duplicates: (1583691, 6)\n",
            "Target distribution:\n",
            "target\n",
            "4    0.501048\n",
            "0    0.498952\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- 2‚Äì4. Preprocessing Pipeline ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1583691/1583691 [00:17<00:00, 90692.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Basic cleaning done in 17.51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1583691/1583691 [02:34<00:00, 10272.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tokenization + Lemmatization done in 154.24s\n",
            "‚úÖ Final dataset shape: (1574120, 9)\n",
            "\n",
            "--- 5. Data Splitting (70/15/15) ---\n",
            "Train: 1101884 | Val: 236118 | Test: 236118\n",
            "\n",
            "‚úÖ Dataset ready for TF-IDF or model training.\n",
            "\n",
            "Example Tweet:\n",
            "Original: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
            "Cleaned: awww thats a bummer you shoulda got david carr of third day to do it d\n",
            "Final: awww thats bummer shoulda got david carr third day\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# Sentiment140 Dataset Loading + Preprocessing (Colab, fixed 'punkt_tab')\n",
        "# =========================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# --- Authenticate with Kaggle ---\n",
        "print(\"--- Authenticating with Kaggle ---\")\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "if os.path.exists(\"/content/kaggle.json\"):\n",
        "    os.system(\"cp /content/kaggle.json /root/.kaggle/\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"‚ùå kaggle.json not found in /content/. Please upload it first!\")\n",
        "\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
        "print(\"‚úÖ Kaggle API key configured successfully.\")\n",
        "\n",
        "# --- Download Sentiment140 dataset ---\n",
        "print(\"\\n--- Downloading Sentiment140 dataset ---\")\n",
        "os.system(\"kaggle datasets download -d kazanova/sentiment140 -p /content --force\")\n",
        "os.system(\"unzip -o /content/sentiment140.zip -d /content/\")\n",
        "\n",
        "# --- Initialize tqdm ---\n",
        "tqdm.pandas()\n",
        "\n",
        "# --- NLTK Resource Download (with punkt_tab fix) ---\n",
        "for pkg in ['stopwords', 'punkt', 'punkt_tab', 'wordnet']:\n",
        "    try:\n",
        "        nltk.data.find(f'corpora/{pkg}')\n",
        "    except LookupError:\n",
        "        nltk.download(pkg, quiet=True)\n",
        "\n",
        "# --- Step 1: Load and Explore Data ---\n",
        "print(\"\\n--- 1. Data Loading & Exploration ---\")\n",
        "COL_NAMES = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
        "FILEPATH = '/content/training.1600000.processed.noemoticon.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(FILEPATH, encoding='ISO-8859-1', names=COL_NAMES)\n",
        "    print(\"‚úÖ Data loaded successfully.\")\n",
        "    df = df.drop_duplicates(subset=['text', 'target'], keep='first')\n",
        "    print(f\"After removing duplicates: {df.shape}\")\n",
        "    print(\"Target distribution:\")\n",
        "    print(df['target'].value_counts(normalize=True))\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\"‚ùå Sentiment140 file not found after download!\")\n",
        "\n",
        "# --- Define Preprocessing Functions ---\n",
        "def basic_text_cleaning(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    text = re.sub(r'@[^\\s]+', '', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize_lemmatize_stopwords(text, stop_words, lemmatizer):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmas = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stop_words and len(tok) > 2]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "# --- Preprocessing Pipeline ---\n",
        "print(\"\\n--- 2‚Äì4. Preprocessing Pipeline ---\")\n",
        "start_time = time.time()\n",
        "df['text_cleaned'] = df['text'].progress_apply(basic_text_cleaning)\n",
        "print(f\"‚úÖ Basic cleaning done in {time.time() - start_time:.2f}s\")\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['im', 'u', 'ur', 'like', 'get', 'go', 'dont'])\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "start_time = time.time()\n",
        "df['text_final'] = df['text_cleaned'].progress_apply(\n",
        "    lambda t: tokenize_lemmatize_stopwords(t, stop_words, lemmatizer)\n",
        ")\n",
        "print(f\"‚úÖ Tokenization + Lemmatization done in {time.time() - start_time:.2f}s\")\n",
        "\n",
        "df['target_encoded'] = df['target'].replace(4, 1)\n",
        "df = df[df['text_final'] != '']\n",
        "print(f\"‚úÖ Final dataset shape: {df.shape}\")\n",
        "\n",
        "# --- Step 5: Split Data ---\n",
        "print(\"\\n--- 5. Data Splitting (70/15/15) ---\")\n",
        "X = df['text_final']\n",
        "y = df['target_encoded']\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
        "print(\"\\n‚úÖ Dataset ready for TF-IDF or model training.\")\n",
        "print(\"\\nExample Tweet:\")\n",
        "print(\"Original:\", df.iloc[0]['text'])\n",
        "print(\"Cleaned:\", df.iloc[0]['text_cleaned'])\n",
        "print(\"Final:\", df.iloc[0]['text_final'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('processed_sentiment140.csv', index=False)\n",
        "print(\"DataFrame saved to processed_sentiment140.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx0dkf897Jdb",
        "outputId": "3d6fa98f-ef7a-47b5-c0ea-384ded3c234f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame saved to processed_sentiment140.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoFS7eCUfsd1"
      },
      "source": [
        "## Task-2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df=pd.read_csv('/content/processed_sentiment140.csv')\n",
        "sample_df = df.sample(n=10000, random_state=42)\n",
        "texts = sample_df[\"text_final\"].astype(str)\n",
        "labels = sample_df[\"target_encoded\"]\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(texts)\n",
        "\n",
        "\n",
        "print(\"‚úÖ TF-IDF matrix shape:\", X_tfidf.shape)\n",
        "\n",
        "\n",
        "try:\n",
        "  pca = PCA(n_components=2, random_state=42)\n",
        "  X_pca = pca.fit_transform(X_tfidf.toarray()[:500])\n",
        "  plt.figure(figsize=(7,5))\n",
        "  plt.scatter(X_pca[:,0], X_pca[:,1], c=labels[:500], cmap='coolwarm', alpha=0.5)\n",
        "  plt.title(\"PCA Visualization of TF-IDF Vectors (500 Samples)\")\n",
        "  plt.xlabel(\"Principal Component 1\")\n",
        "  plt.ylabel(\"Principal Component 2\")\n",
        "  plt.show()\n",
        "except Exception as e:\n",
        "  print(f\"‚ö†Ô∏è PCA visualization skipped: {e}\")\n",
        "\n",
        "\n",
        "print(\"Top 10 TF-IDF features:\")\n",
        "print(vectorizer.get_feature_names_out()[:10])\n",
        "\n",
        "\n",
        "sample_df.to_csv('sample_df.csv', index=False)\n",
        "print(\"DataFrame saved to sample_df.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "OnhJcQoa2vL0",
        "outputId": "a69bc5bf-46a3-43cd-a1b3-d65812003fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/processed_sentiment140.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4066672314.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/processed_sentiment140.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_final\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/processed_sentiment140.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUpFIQJ3fwgO"
      },
      "source": [
        "## Task-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "d6c5ed95",
        "outputId": "89b291b7-8388-49e5-b3bf-a1aa19339e74"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_df.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4246020174.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_final\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_encoded\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_df.csv'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "\n",
        "sample_df = pd.read_csv('/content/sample_df.csv')\n",
        "texts = sample_df[\"text_final\"].astype(str)\n",
        "labels = sample_df[\"target_encoded\"].astype(int)\n",
        "\n",
        "\n",
        "max_words = 5000\n",
        "max_len = 50\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(padded, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "Embedding(max_words, 64, input_length=max_len),\n",
        "Bidirectional(LSTM(64, return_sequences=False)),\n",
        "Dropout(0.4),\n",
        "Dense(32, activation='relu'),\n",
        "Dropout(0.3),\n",
        "Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "X_train, y_train,\n",
        "validation_data=(X_val, y_val),\n",
        "epochs=3,\n",
        "batch_size=64,\n",
        "verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task-4"
      ],
      "metadata": {
        "id": "PvZNTyh52NkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# ‚úÖ Task 4 ‚Äî Feature Selection using Ant Colony Optimization (ACO)\n",
        "# =========================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"\\n--- Task 4: Feature Selection using Ant Colony Optimization (ACO) ---\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Step 1Ô∏è‚É£ Load data (same as used in Task 3)\n",
        "# ----------------------------------------------------\n",
        "sample_df = pd.read_csv('/content/sample_df.csv')\n",
        "\n",
        "texts = sample_df[\"text_final\"].astype(str)\n",
        "labels = sample_df[\"target_encoded\"].astype(int)\n",
        "\n",
        "# Train/Val/Test Split (same ratios)\n",
        "X_train_texts, X_temp_texts, y_train, y_temp = train_test_split(texts, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "X_val_texts, X_test_texts, y_val, y_test = train_test_split(X_temp_texts, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Step 2Ô∏è‚É£ TF-IDF Representation\n",
        "# ----------------------------------------------------\n",
        "max_features = 30000  # keep it reasonable for ACO\n",
        "tfidf = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_texts)\n",
        "X_val_tfidf = tfidf.transform(X_val_texts)\n",
        "X_test_tfidf = tfidf.transform(X_test_texts)\n",
        "feature_names = np.array(tfidf.get_feature_names_out())\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Step 3Ô∏è‚É£ Define helper: evaluate subset accuracy\n",
        "# ----------------------------------------------------\n",
        "def evaluate_subset(subset_indices, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Train a logistic regression model on a subset of features and return validation accuracy.\"\"\"\n",
        "    if len(subset_indices) == 0:\n",
        "        return 0\n",
        "    model = LogisticRegression(max_iter=300)\n",
        "    model.fit(X_train[:, subset_indices], y_train)\n",
        "    preds = model.predict(X_val[:, subset_indices])\n",
        "    return accuracy_score(y_val, preds)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Step 4Ô∏è‚É£ Define ACO function (no classes)\n",
        "# ----------------------------------------------------\n",
        "def ant_colony_feature_selection(n_features, X_train, y_train, X_val, y_val, n_ants=5, n_iter=3, evap_rate=0.3):\n",
        "    pheromone = np.ones(n_features)\n",
        "    best_subset, best_acc = [], 0\n",
        "\n",
        "    for it in range(n_iter):\n",
        "        print(f\"\\nIteration {it+1}/{n_iter}\")\n",
        "        all_subsets, all_accuracies = [], []\n",
        "\n",
        "        for ant in range(n_ants):\n",
        "            probs = pheromone / np.sum(pheromone)\n",
        "            chosen = np.random.rand(n_features) < probs\n",
        "            subset = np.where(chosen)[0]\n",
        "\n",
        "            # Always ensure a minimum subset size\n",
        "            if len(subset) < 5:\n",
        "                subset = np.random.choice(n_features, size=10, replace=False)\n",
        "\n",
        "            acc = evaluate_subset(subset, X_train, y_train, X_val, y_val)\n",
        "            all_subsets.append(subset)\n",
        "            all_accuracies.append(acc)\n",
        "            print(f\"üêú Ant {ant+1}: selected {len(subset)} features, acc={acc:.4f}\")\n",
        "\n",
        "        # Best ant in this iteration\n",
        "        best_idx = np.argmax(all_accuracies)\n",
        "        iteration_best_subset = all_subsets[best_idx]\n",
        "        iteration_best_acc = all_accuracies[best_idx]\n",
        "\n",
        "        # Pheromone update\n",
        "        pheromone = (1 - evap_rate) * pheromone\n",
        "        pheromone[iteration_best_subset] += evap_rate * iteration_best_acc\n",
        "\n",
        "        # Update global best\n",
        "        if iteration_best_acc > best_acc:\n",
        "            best_acc = iteration_best_acc\n",
        "            best_subset = iteration_best_subset\n",
        "\n",
        "        print(f\"Iteration best acc: {iteration_best_acc:.4f}, Global best acc: {best_acc:.4f}\")\n",
        "\n",
        "    return best_subset, best_acc\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Step 5Ô∏è‚É£ Run ACO\n",
        "# ----------------------------------------------------\n",
        "start_time = time.time()\n",
        "selected_indices, aco_best_acc = ant_colony_feature_selection(\n",
        "    X_train_tfidf.shape[1],\n",
        "    X_train_tfidf, y_train,\n",
        "    X_val_tfidf, y_val,\n",
        "    n_ants=5, n_iter=3\n",
        ")\n",
        "end_time = time.time()\n",
        "\n",
        "selected_features = feature_names[selected_indices]\n",
        "print(f\"\\n‚úÖ ACO selected {len(selected_features)} features\")\n",
        "print(\"Example selected features:\", selected_features[:20])\n",
        "print(f\"ACO Time: {end_time - start_time:.2f}s\")\n",
        "print(f\"ACO Validation Accuracy: {aco_best_acc:.4f}\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Step 6Ô∏è‚É£ Compare Model Performance Before vs After ACO\n",
        "# ----------------------------------------------------\n",
        "# Before ACO (all features)\n",
        "base_model = LogisticRegression(max_iter=300)\n",
        "base_model.fit(X_train_tfidf, y_train)\n",
        "base_preds = base_model.predict(X_test_tfidf)\n",
        "base_acc = accuracy_score(y_test, base_preds)\n",
        "base_f1 = f1_score(y_test, base_preds)\n",
        "\n",
        "# After ACO (selected features)\n",
        "aco_model = LogisticRegression(max_iter=300)\n",
        "aco_model.fit(X_train_tfidf[:, selected_indices], y_train)\n",
        "aco_preds = aco_model.predict(X_test_tfidf[:, selected_indices])\n",
        "aco_acc = accuracy_score(y_test, aco_preds)\n",
        "aco_f1 = f1_score(y_test, aco_preds)\n",
        "\n",
        "print(\"\\n--- Final Model Comparison ---\")\n",
        "print(f\"Before ACO: Accuracy={base_acc:.4f}, F1={base_f1:.4f}\")\n",
        "print(f\"After  ACO: Accuracy={aco_acc:.4f}, F1={aco_f1:.4f}\")\n",
        "print(f\"ACO Selected {len(selected_features)} / {len(feature_names)} features ({len(selected_features)/len(feature_names)*100:.1f}%)\")\n"
      ],
      "metadata": {
        "id": "7XBjVH752PJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "224c761f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XqH1xvSEpE4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0YORoswFCjK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiC6LvsfEm0k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}